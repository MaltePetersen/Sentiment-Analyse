{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Sentiment Analyse von McDonalds Reviews und IMBD Daten"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zielsetzung \n",
    "In diesem Notebook erweitern wir die McDonald's-Daten mit IMDB-Daten, um die Auswirkungen auf die Genauigkeit zu erforschen. Dafür teilen wir die Daten in positive und negative Reviews auf, was zu nur zwei Labels führt. Die Baseline basierend auf dem gesunden Menschenverstand liegt daher bei einer Genauigkeit von 50%.\n",
    " Unser Ziel ist es herauszufinden, ob wir durch Hinzufügen immer mehr Daten eine allgemeine Semantikanalyse erreichen können, die auf jede beliebige Review angewendet werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 12:31:59.030006: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-09 12:31:59.057791: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-09 12:31:59.470662: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenvorbereitung inkl. Behandlung bekannter Probleme"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laden der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping McDonalds dataset...\n",
      "/usr/bin/zsh: /home/x/miniconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /usr/bin/zsh)\n",
      "Archive:  mcdonalds-store-reviews.zip\n",
      "Unzipping IMDB dataset...\n",
      "/usr/bin/zsh: /home/x/miniconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /usr/bin/zsh)\n",
      "Archive:  imdb-dataset-of-50k-movie-reviews.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Download McDonalds dataset\n",
    "if not os.path.exists('mcdonalds-store-reviews.zip'):\n",
    "    print(\"Downloading McDonalds dataset...\")\n",
    "    !kaggle datasets download -d nelgiriyewithana/mcdonalds-store-reviews\n",
    "if os.path.exists('mcdonalds-store-reviews.zip'):\n",
    "    print(\"Unzipping McDonalds dataset...\")\n",
    "    !unzip -n mcdonalds-store-reviews.zip\n",
    "\n",
    "# Download IMDB dataset\n",
    "if not os.path.exists('imdb-dataset-of-50k-movie-reviews.zip'):\n",
    "    print(\"Downloading IMDB dataset...\")\n",
    "    !kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "if os.path.exists('imdb-dataset-of-50k-movie-reviews.zip'):\n",
    "    print(\"Unzipping IMDB dataset...\")\n",
    "    !unzip -n imdb-dataset-of-50k-movie-reviews.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mc = pd.read_csv('McDonald_s_Reviews.csv', encoding=\"latin-1\")\n",
    "df_imdb = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>store_name</th>\n",
       "      <th>category</th>\n",
       "      <th>store_address</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>rating_count</th>\n",
       "      <th>review_time</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>13749 US-183 Hwy, Austin, TX 78750, United States</td>\n",
       "      <td>30.460718</td>\n",
       "      <td>-97.792874</td>\n",
       "      <td>1,240</td>\n",
       "      <td>3 months ago</td>\n",
       "      <td>Why does it look like someone spit on my food?...</td>\n",
       "      <td>1 star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>13749 US-183 Hwy, Austin, TX 78750, United States</td>\n",
       "      <td>30.460718</td>\n",
       "      <td>-97.792874</td>\n",
       "      <td>1,240</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>It'd McDonalds. It is what it is as far as the...</td>\n",
       "      <td>4 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>13749 US-183 Hwy, Austin, TX 78750, United States</td>\n",
       "      <td>30.460718</td>\n",
       "      <td>-97.792874</td>\n",
       "      <td>1,240</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>Made a mobile order got to the speaker and che...</td>\n",
       "      <td>1 star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>13749 US-183 Hwy, Austin, TX 78750, United States</td>\n",
       "      <td>30.460718</td>\n",
       "      <td>-97.792874</td>\n",
       "      <td>1,240</td>\n",
       "      <td>a month ago</td>\n",
       "      <td>My mc. Crispy chicken sandwich was ï¿½ï¿½ï¿½ï¿...</td>\n",
       "      <td>5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>13749 US-183 Hwy, Austin, TX 78750, United States</td>\n",
       "      <td>30.460718</td>\n",
       "      <td>-97.792874</td>\n",
       "      <td>1,240</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>I repeat my order 3 times in the drive thru, a...</td>\n",
       "      <td>1 star</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviewer_id  store_name              category  \\\n",
       "0            1  McDonald's  Fast food restaurant   \n",
       "1            2  McDonald's  Fast food restaurant   \n",
       "2            3  McDonald's  Fast food restaurant   \n",
       "3            4  McDonald's  Fast food restaurant   \n",
       "4            5  McDonald's  Fast food restaurant   \n",
       "\n",
       "                                       store_address  latitude   longitude  \\\n",
       "0  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
       "1  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
       "2  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
       "3  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
       "4  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
       "\n",
       "  rating_count   review_time  \\\n",
       "0        1,240  3 months ago   \n",
       "1        1,240    5 days ago   \n",
       "2        1,240    5 days ago   \n",
       "3        1,240   a month ago   \n",
       "4        1,240  2 months ago   \n",
       "\n",
       "                                              review   rating  \n",
       "0  Why does it look like someone spit on my food?...   1 star  \n",
       "1  It'd McDonalds. It is what it is as far as the...  4 stars  \n",
       "2  Made a mobile order got to the speaker and che...   1 star  \n",
       "3  My mc. Crispy chicken sandwich was ï¿½ï¿½ï¿½ï¿...  5 stars  \n",
       "4  I repeat my order 3 times in the drive thru, a...   1 star  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bereinigung der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m clean_text\n\u001b[1;32m      2\u001b[0m df_mc[\u001b[39m'\u001b[39m\u001b[39mreview\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_mc[\u001b[39m'\u001b[39m\u001b[39mreview\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(clean_text)\n\u001b[0;32m----> 3\u001b[0m df_imdb[\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_imdb[\u001b[39m'\u001b[39;49m\u001b[39mreview\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(clean_text)\n\u001b[1;32m      5\u001b[0m df_mc \u001b[39m=\u001b[39m df_mc[[\u001b[39m'\u001b[39m\u001b[39mreview\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrating\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m      6\u001b[0m df_imdb \u001b[39m=\u001b[39m df_imdb[[\u001b[39m'\u001b[39m\u001b[39mreview\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1077\u001b[0m             values,\n\u001b[1;32m   1078\u001b[0m             f,\n\u001b[1;32m   1079\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1080\u001b[0m         )\n\u001b[1;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Development/Sentiment-Anayses-zu-Text-Generation/utils/text_utils.py:14\u001b[0m, in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     12\u001b[0m test \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m<.*?>\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, test)\n\u001b[1;32m     13\u001b[0m \u001b[39m# Remove stopwords\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m test \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m word_tokenize(test) \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)])\n\u001b[1;32m     15\u001b[0m \u001b[39mreturn\u001b[39;00m test\n",
      "File \u001b[0;32m~/Development/Sentiment-Anayses-zu-Text-Generation/utils/text_utils.py:14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m test \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m<.*?>\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, test)\n\u001b[1;32m     13\u001b[0m \u001b[39m# Remove stopwords\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m test \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m word_tokenize(test) \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m'\u001b[39;49m\u001b[39menglish\u001b[39;49m\u001b[39m'\u001b[39;49m)])\n\u001b[1;32m     15\u001b[0m \u001b[39mreturn\u001b[39;00m test\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/nltk/corpus/reader/wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwords\u001b[39m(\u001b[39mself\u001b[39m, fileids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, ignore_lines_startswith\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m     20\u001b[0m         line\n\u001b[0;32m---> 21\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m line_tokenize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw(fileids))\n\u001b[1;32m     22\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line\u001b[39m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[1;32m     23\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/nltk/tokenize/simple.py:137\u001b[0m, in \u001b[0;36mline_tokenize\u001b[0;34m(text, blanklines)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mline_tokenize\u001b[39m(text, blanklines\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdiscard\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m LineTokenizer(blanklines)\u001b[39m.\u001b[39;49mtokenize(text)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/nltk/tokenize/simple.py:116\u001b[0m, in \u001b[0;36mLineTokenizer.tokenize\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39m# If requested, strip off blank lines.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blanklines \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdiscard\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 116\u001b[0m     lines \u001b[39m=\u001b[39m [l \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lines \u001b[39mif\u001b[39;00m l\u001b[39m.\u001b[39mrstrip()]\n\u001b[1;32m    117\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blanklines \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdiscard-eof\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m lines \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m lines[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mstrip():\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/nltk/tokenize/simple.py:116\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39m# If requested, strip off blank lines.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blanklines \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdiscard\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 116\u001b[0m     lines \u001b[39m=\u001b[39m [l \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lines \u001b[39mif\u001b[39;00m l\u001b[39m.\u001b[39;49mrstrip()]\n\u001b[1;32m    117\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blanklines \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdiscard-eof\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m lines \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m lines[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mstrip():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils.text_utils import clean_text\n",
    "df_mc['review'] = df_mc['review'].apply(clean_text)\n",
    "df_imdb['sentiment'] = df_imdb['review'].apply(clean_text)\n",
    "\n",
    "df_mc = df_mc[['review', 'rating']]\n",
    "df_imdb = df_imdb[['review', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>look like someone spit food normal transaction...</td>\n",
       "      <td>1 star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>itd mcdonalds far food atmosphere go staff mak...</td>\n",
       "      <td>4 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>made mobile order got speaker checked line mov...</td>\n",
       "      <td>1 star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mc crispy chicken sandwich customer service qu...</td>\n",
       "      <td>5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>repeat order times drive thru still manage mes...</td>\n",
       "      <td>1 star</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review   rating\n",
       "0  look like someone spit food normal transaction...   1 star\n",
       "1  itd mcdonalds far food atmosphere go staff mak...  4 stars\n",
       "2  made mobile order got speaker checked line mov...   1 star\n",
       "3  mc crispy chicken sandwich customer service qu...  5 stars\n",
       "4  repeat order times drive thru still manage mes...   1 star"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imdb.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Löschen der 3 Sterne Bewertungen beim McDonalds Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mc = df_mc[df_mc['rating'] != '3 stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_mc.columns)\n",
    "print(df_imdb.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konvertierung der train data zu numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mc = df_mc['review'].to_numpy()\n",
    "data_imdb = df_imdb['review'].to_numpy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konvertierung der Datasets von string labels zu Nummern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_mapping_imdb = {\n",
    "    'positive': 1,\n",
    "    'negative': 0,\n",
    "}\n",
    "\n",
    "label_imdb = df_imdb['sentiment'].map(rating_mapping_imdb).to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_mapping_mc = {\n",
    "    '1 star': 0,\n",
    "    '2 stars': 0,\n",
    "    '4 stars': 1,\n",
    "    '5 stars': 1\n",
    "}\n",
    "\n",
    "label_mc = df_mc['rating'].map(rating_mapping_mc).to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten zusammenführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.append(data_imdb, data_mc)\n",
    "label = np.append(label_imdb,label_mc)\n",
    "print(data.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufteilung in Trainings, Validierungs und Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_label, test_label = train_test_split(data, label, test_size=0.2, random_state=42)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### McDonalds Train/Test split\n",
    "\n",
    "Um die Veränderung des Ergbnisses, durch das Einführen von Testsdaten vergleichen zu können, werden wir später auch noch ein Model nur mit den McD Daten trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_mc, test_data_mc, train_label_mc, test_label_mc = train_test_split(data_mc, label_mc, test_size=0.2, random_state=42)\n",
    "\n",
    "print(train_data_mc.shape)\n",
    "print(test_data_mc.shape)\n",
    "print(train_label_mc.shape)\n",
    "print(test_label_mc.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB Train/Test split\n",
    "\n",
    "Um die Veränderung des Ergbnisses, durch das Einführen von Testsdaten vergleichen zu können, werden wir später auch noch ein Model nur mit den McD Daten trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_imdb, test_data_imdb, train_label_imdb, test_label_imdb = train_test_split(data_imdb, label_imdb, test_size=0.2, random_state=42)\n",
    "\n",
    "print(train_data_imdb.shape)\n",
    "print(test_data_imdb.shape)\n",
    "print(train_label_imdb.shape)\n",
    "print(test_label_imdb.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text zu Dictonary Repräsentationen umbauen\n",
    "Erklärung Menge an Tokens\n",
    "Erklärung ngrams \n",
    "Erklärung multi_hot und warum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    " max_tokens=20000,\n",
    "ngrams=2,\n",
    " output_mode=\"multi_hot\",\n",
    " standardize=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(train_data) \n",
    "text_vectorization.adapt(test_data) \n",
    "\n",
    "multi_hot_train_data = text_vectorization(train_data)\n",
    "multi_hot_test_data = text_vectorization(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Vectorization für McD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_hot_train_data_mc = text_vectorization(train_data_mc)\n",
    "multi_hot_test_data_mc = text_vectorization(test_data_mc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Vectorization für IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_hot_train_data_imdb = text_vectorization(train_data_imdb)\n",
    "multi_hot_test_data_imdb = text_vectorization(test_data_imdb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erstellung eines entsprechenden künstlichen neuronalen Models sowie geeigente Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(max_tokens=20000, hidden_dim=64):\n",
    " inputs = keras.Input(shape=(max_tokens,))\n",
    " x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    " x = layers.Dense(32, activation=\"relu\")(x)\n",
    " x = layers.Dropout(0.5)(x)\n",
    " x = layers.Dense(16, activation=\"relu\")(x)\n",
    " outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    " model = keras.Model(inputs, outputs)\n",
    " model.compile(optimizer=\"rmsprop\",\n",
    " loss=\"binary_crossentropy\",\n",
    " metrics=[\"accuracy\"], \n",
    " )\n",
    " return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",  # Metric to monitor\n",
    "        patience=3,  # Number of epochs with no improvement after which training will be stopped\n",
    "        restore_best_weights=True,  # Restore the weights of the best epoch\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(multi_hot_train_data,train_label,\n",
    "validation_split=0.2,\n",
    " epochs=10, \n",
    " callbacks= callbacks\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history_dict = history.history\n",
    "loss_values = history_dict[\"loss\"]\n",
    "val_loss_values = history_dict[\"val_loss\"]\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluierung auf alle Daten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(multi_hot_test_data, test_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalurierung nur auf die McDonalds Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(multi_hot_test_data_mc, test_label_mc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalurierung nur auf die IMDB Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(multi_hot_test_data_imdb, test_label_imdb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vergleich nur McDonalds Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mc = get_model()\n",
    "model_mc.fit(multi_hot_train_data_mc,train_label_mc,\n",
    "validation_split=0.2,\n",
    " epochs=10, \n",
    " callbacks= callbacks\n",
    " )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McDonalds Model auf McDonalds Model Testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mc.evaluate(multi_hot_test_data_mc, test_label_mc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McDonalds Model auf IMDB Daten testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mc.evaluate(multi_hot_test_data_imdb, test_label_imdb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McDonalds Model auf allgemeine Daten testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mc.evaluate(multi_hot_test_data, test_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vergleich IMDB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_imdb = get_model()\n",
    "model_imdb.fit(multi_hot_train_data_imdb,train_label_imdb,\n",
    "validation_split=0.2,\n",
    " epochs=10, \n",
    " callbacks= callbacks\n",
    " )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Model auf IMDB Model Testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_imdb.evaluate(multi_hot_test_data_imdb, test_label_imdb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Model auf McDonalds Daten Testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_imdb.evaluate(multi_hot_test_data_mc, test_label_mc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Model auf allgemeines Datenset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_imdb.evaluate(multi_hot_test_data, test_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Mehr Daten ermöglichen die Lösung allgemeinerer Probleme, führen jedoch zu einer verringerten Genauigkeit bei spezifischeren Modellen. Ein Problem in unserem Modell besteht darin, dass wir nicht gleich viele Trainingsdaten von IMDB und McDonald's haben, wodurch das Modell stärker auf Movie Reviews gewichtet wird.\n",
    "\n",
    "Das allgemeine Modell erzielt eine Genauigkeit von 90% auf den allgemeinen Daten, aber bei der Evaluierung nur auf die McDonald's- oder IMDB-Reviews schafft es knapp oder gar nicht, die Common-Sense-Baseline zu übertreffen. Dennoch sind die Ergebnisse hier etwas besser als bei der Verwendung des IMDB-Modells auf den McDonald's-Daten oder des McDonald's-Modells auf den IMDB-Daten. Daher wäre anzunehmen, dass die Genauigkeit steigen würde, wenn wir mehr Reviews hinzufügen (z. B. Amazon-Reviews). Dadurch könnte langfristig eine semantische Analyse für alle Arten von Reviews erstellt werden."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
