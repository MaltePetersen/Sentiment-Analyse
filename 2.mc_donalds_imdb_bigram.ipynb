{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Sentiment Analyse von McDonalds Reviews und IMBD Daten"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zielsetzung \n",
    "In diesem Notebook erweitern wir die McDonald's-Daten mit IMDB-Daten, um die Auswirkungen auf die Genauigkeit zu erforschen. Dafür teilen wir die Daten in positive und negative Reviews auf, was zu nur zwei Labels führt. Die Baseline basierend auf dem gesunden Menschenverstand liegt daher bei einer Genauigkeit von 50%.\n",
    " Unser Ziel ist es herauszufinden, ob wir durch Hinzufügen immer mehr Daten eine allgemeine Semantikanalyse erreichen können, die auf jede beliebige Review angewendet werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenvorbereitung inkl. Behandlung bekannter Probleme"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laden der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Download McDonalds dataset\n",
    "if not os.path.exists('mcdonalds-store-reviews.zip'):\n",
    "    print(\"Downloading McDonalds dataset...\")\n",
    "    !kaggle datasets download -d nelgiriyewithana/mcdonalds-store-reviews\n",
    "if os.path.exists('mcdonalds-store-reviews.zip'):\n",
    "    print(\"Unzipping McDonalds dataset...\")\n",
    "    !unzip -n mcdonalds-store-reviews.zip\n",
    "\n",
    "# Download IMDB dataset\n",
    "if not os.path.exists('imdb-dataset-of-50k-movie-reviews.zip'):\n",
    "    print(\"Downloading IMDB dataset...\")\n",
    "    !kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "if os.path.exists('imdb-dataset-of-50k-movie-reviews.zip'):\n",
    "    print(\"Unzipping IMDB dataset...\")\n",
    "    !unzip -n imdb-dataset-of-50k-movie-reviews.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mc = pd.read_csv('McDonald_s_Reviews.csv', encoding=\"latin-1\")\n",
    "df_imdb = pd.read_csv('IMDB Dataset.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Löschen der 3 Sterne Bewertungen beim McDonalds Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mc = df_mc[df_mc['rating'] != '3 stars']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_mc.columns)\n",
    "print(df_imdb.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konvertierung der train data zu numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mc = df_mc['review'].to_numpy()\n",
    "data_imdb = df_imdb['review'].to_numpy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konvertierung der Datasets von string labels zu Nummern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_mapping_imdb = {\n",
    "    'positive': 1,\n",
    "    'negative': 0,\n",
    "}\n",
    "\n",
    "label_imdb = df_imdb['sentiment'].map(rating_mapping_imdb).to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_mapping_mc = {\n",
    "    '1 star': 0,\n",
    "    '2 stars': 0,\n",
    "    '4 stars': 1,\n",
    "    '5 stars': 1\n",
    "}\n",
    "\n",
    "label_mc = df_mc['rating'].map(rating_mapping_mc).to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten zusammenführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.append(data_imdb, data_mc)\n",
    "label = np.append(label_imdb,label_mc)\n",
    "print(data.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufteilung in Trainings, Validierungs und Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_label, test_label = train_test_split(data, label, test_size=0.2, random_state=42)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### McDonalds Train/Test split\n",
    "\n",
    "Um die Veränderung des Ergbnisses, durch das Einführen von Testsdaten vergleichen zu können, werden wir später auch noch ein Model nur mit den McD Daten trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_mc, test_data_mc, train_label_mc, test_label_mc = train_test_split(data_mc, label_mc, test_size=0.2, random_state=42)\n",
    "\n",
    "print(train_data_mc.shape)\n",
    "print(test_data_mc.shape)\n",
    "print(train_label_mc.shape)\n",
    "print(test_label_mc.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB Train/Test split\n",
    "\n",
    "Um die Veränderung des Ergbnisses, durch das Einführen von Testsdaten vergleichen zu können, werden wir später auch noch ein Model nur mit den McD Daten trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_imdb, test_data_imdb, train_label_imdb, test_label_imdb = train_test_split(data_imdb, label_imdb, test_size=0.2, random_state=42)\n",
    "\n",
    "print(train_data_imdb.shape)\n",
    "print(test_data_imdb.shape)\n",
    "print(train_label_imdb.shape)\n",
    "print(test_label_imdb.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text zu Dictonary Repräsentationen umbauen\n",
    "Erklärung Menge an Tokens\n",
    "Erklärung ngrams \n",
    "Erklärung multi_hot und warum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    " max_tokens=30000,\n",
    "ngrams=3,\n",
    " output_mode=\"multi_hot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(train_data) \n",
    "text_vectorization.adapt(test_data) \n",
    "\n",
    "multi_hot_train_data = text_vectorization(train_data)\n",
    "multi_hot_test_data = text_vectorization(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Vectorization für McD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization_mc = TextVectorization(\n",
    " max_tokens=30000,\n",
    "ngrams=3,\n",
    " output_mode=\"multi_hot\",\n",
    ")\n",
    "\n",
    "text_vectorization_mc.adapt(train_data_mc) \n",
    "text_vectorization_mc.adapt(test_data_mc) \n",
    "\n",
    "multi_hot_train_data_mc = text_vectorization_mc(train_data_mc)\n",
    "multi_hot_test_data_mc = text_vectorization_mc(test_data_mc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Vectorization für IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization_imdb = TextVectorization(\n",
    " max_tokens=30000,\n",
    "ngrams=3,\n",
    " output_mode=\"multi_hot\",\n",
    ")\n",
    "\n",
    "text_vectorization_imdb.adapt(train_data_imdb) \n",
    "text_vectorization_imdb.adapt(test_data_imdb) \n",
    "\n",
    "multi_hot_train_data_imdb = text_vectorization_imdb(train_data_imdb)\n",
    "multi_hot_test_data_imdb = text_vectorization_imdb(test_data_imdb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erstellung eines entsprechenden künstlichen neuronalen Models sowie geeigente Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(max_tokens=30000, hidden_dim=64):\n",
    " inputs = keras.Input(shape=(max_tokens,))\n",
    " x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    " x = layers.Dense(32, activation=\"relu\")(x)\n",
    " x = layers.Dropout(0.5)(x)\n",
    " x = layers.Dense(16, activation=\"relu\")(x)\n",
    " outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    " model = keras.Model(inputs, outputs)\n",
    " model.compile(optimizer=\"rmsprop\",\n",
    " loss=\"binary_crossentropy\",\n",
    " metrics=[\"accuracy\"], \n",
    " )\n",
    " return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",  # Metric to monitor\n",
    "        patience=3,  # Number of epochs with no improvement after which training will be stopped\n",
    "        restore_best_weights=True,  # Restore the weights of the best epoch\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(multi_hot_train_data,train_label,\n",
    "validation_split=0.2,\n",
    " epochs=10, \n",
    " callbacks= callbacks\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history_dict = history.history\n",
    "loss_values = history_dict[\"loss\"]\n",
    "val_loss_values = history_dict[\"val_loss\"]\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluierung auf alle Daten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(multi_hot_test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_hot_test_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalurierung nur auf die McDonalds Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(multi_hot_test_data_mc, test_label_mc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalurierung nur auf die IMDB Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(multi_hot_test_data_imdb, test_label_imdb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vergleich nur McDonalds Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mc = get_model()\n",
    "model_mc.fit(multi_hot_train_data_mc,train_label_mc,\n",
    "validation_split=0.2,\n",
    " epochs=10, \n",
    " callbacks= callbacks\n",
    " )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McDonalds Model auf McDonalds Model Testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mc.evaluate(multi_hot_test_data_mc, test_label_mc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McDonalds Model auf IMDB Daten testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mc.evaluate(multi_hot_test_data_imdb, test_label_imdb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McDonalds Model auf allgemeine Daten testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mc.evaluate(multi_hot_test_data, test_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vergleich IMDB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_imdb = get_model()\n",
    "model_imdb.fit(multi_hot_train_data_imdb,train_label_imdb,\n",
    "validation_split=0.2,\n",
    " epochs=10, \n",
    " callbacks= callbacks\n",
    " )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Model auf IMDB Model Testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_imdb.evaluate(multi_hot_test_data_imdb, test_label_imdb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Model auf McDonalds Daten Testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_imdb.evaluate(multi_hot_test_data_mc, test_label_mc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Model auf allgemeines Datenset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_imdb.evaluate(multi_hot_test_data, test_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Mehr Daten ermöglichen die Lösung allgemeinerer Probleme, führen jedoch zu einer verringerten Genauigkeit bei spezifischeren Modellen. Ein Problem in unserem Modell besteht darin, dass wir nicht gleich viele Trainingsdaten von IMDB und McDonald's haben, wodurch das Modell stärker auf Movie Reviews gewichtet wird.\n",
    "\n",
    "Das allgemeine Modell erzielt eine Genauigkeit von 90% auf den allgemeinen Daten, aber bei der Evaluierung nur auf die McDonald's- oder IMDB-Reviews schafft es knapp oder gar nicht, die Common-Sense-Baseline zu übertreffen. Dennoch sind die Ergebnisse hier etwas besser als bei der Verwendung des IMDB-Modells auf den McDonald's-Daten oder des McDonald's-Modells auf den IMDB-Daten. Daher wäre anzunehmen, dass die Genauigkeit steigen würde, wenn wir mehr Reviews hinzufügen (z. B. Amazon-Reviews). Dadurch könnte langfristig eine semantische Analyse für alle Arten von Reviews erstellt werden."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
