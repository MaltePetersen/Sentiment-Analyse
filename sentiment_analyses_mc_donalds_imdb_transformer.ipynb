{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.layers import TextVectorization\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from plotly.offline import init_notebook_mode\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams['figure.figsize'] = [20, 8]\n",
    "plt.rcParams['font.size'] = 18 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, Visualize and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Download McDonalds dataset\n",
    "if not os.path.exists('mcdonalds-store-reviews.zip'):\n",
    "    print(\"Downloading McDonalds dataset...\")\n",
    "    !kaggle datasets download -d nelgiriyewithana/mcdonalds-store-reviews\n",
    "if os.path.exists('mcdonalds-store-reviews.zip'):\n",
    "    print(\"Unzipping McDonalds dataset...\")\n",
    "    !unzip -n mcdonalds-store-reviews.zip\n",
    "\n",
    "# Download IMDB dataset\n",
    "if not os.path.exists('imdb-dataset-of-50k-movie-reviews.zip'):\n",
    "    print(\"Downloading IMDB dataset...\")\n",
    "    !kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "if os.path.exists('imdb-dataset-of-50k-movie-reviews.zip'):\n",
    "    print(\"Unzipping IMDB dataset...\")\n",
    "    !unzip -n imdb-dataset-of-50k-movie-reviews.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mc = pd.read_csv('McDonald_s_Reviews.csv', encoding=\"latin-1\")\n",
    "df_imdb = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting McDonalds review labels from categorical to binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mc = df_mc[df_mc['rating'] != '3 stars']\n",
    "rating_mapping_mc = {\n",
    "    '1 star': 0,\n",
    "    '2 stars': 0,\n",
    "    '4 stars': 1,\n",
    "    '5 stars': 1\n",
    "}\n",
    "\n",
    "label_mc = df_mc['rating'].map(rating_mapping_mc).to_numpy()\n",
    "print(label_mc[:10])\n",
    "print(f'Labels McDonalds: {len(label_mc)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting IMDB review labels from categorical to binary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_imdb = df_imdb['sentiment'].map({'positive': 1, 'negative': 0}).to_numpy()\n",
    "print(label_imdb[:10])\n",
    "print(f'Labels IMDB: {len(label_imdb)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mc = df_mc['review'].to_numpy()\n",
    "data_imdb = df_imdb['review'].to_numpy()\n",
    "data = np.append(data_imdb, data_mc)\n",
    "label = np.append(label_imdb, label_mc)\n",
    "print(f'Reviews: {len(data)}')\n",
    "print(f'Labels: {len(label)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(x=['positive', 'negative'], y=[len(label[label == 1]), len(label[label == 0])], title='Overall review distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(x=['positive', 'negative'], y=[len(label_mc[label_mc == 1]), len(label_mc[label_mc == 0])], title='McDonalds review distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(x=['positive', 'negative'], y=[len(label_imdb[label_imdb == 1]), len(label_imdb[label_imdb == 0])], title='IMDB review distribution')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train/validation/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_label, test_label = train_test_split(data, label, test_size=0.2, random_state=42)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_data.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization / Vectorization / Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_FEATURES = 30000 \n",
    "SEQ_LENGTH = 100\n",
    "EMBEDDING_DIM = 100 \n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = TextVectorization(max_tokens=MAX_FEATURES, output_sequence_length=SEQ_LENGTH)\n",
    "text_vectorizer.adapt(train_data)\n",
    "text_vectorizer.adapt(test_data)\n",
    "\n",
    "# Check the vocabulary\n",
    "vocabulary = text_vectorizer.get_vocabulary()\n",
    "vocab_size = len(vocabulary)\n",
    "print(f'Vocabulary size: {len(vocabulary)}')\n",
    "print(f'First 10 Vocabulary Item: {vocabulary[:10]}')\n",
    "\n",
    "# TODO We already have some bs in this vocabulary if we don't enforce the max_features => need better cleaning\n",
    "print(f'Last 10 Vocabulary Item: {vocabulary[-10:]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Reviews to SEQ_LENGTH words (padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = text_vectorizer(train_data)\n",
    "test_data = text_vectorizer(test_data)\n",
    "\n",
    "print(f'Train data shape: {train_data.shape}')\n",
    "print(f'Test data shape: {test_data.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only want to classify our reviews into either positive or negative sentiment, we don't need to use the full transformer architecture. The encoder block is sufficient. Via. [Attention is all you need](https://arxiv.org/abs/1706.03762)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, heads, neurons):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(neurons, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(0.5)\n",
    "        self.dropout2 = layers.Dropout(0.5)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, embedding_matrix=None):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        if embedding_matrix is not None:\n",
    "            self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, weights=[embedding_matrix], trainable=False)\n",
    "        else:\n",
    "            self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "NUM_HEADS = 16  # number of attention heads\n",
    "FFN_DIM = 1024 # hidden layer size in feed forward network inside transformer\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embedding_matrix=None):\n",
    "    inputs = layers.Input(shape=(SEQ_LENGTH,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(SEQ_LENGTH, MAX_FEATURES, EMBEDDING_DIM)   \n",
    "    if(embedding_matrix is not None) : \n",
    "        embedding_layer = layers.Embedding(MAX_FEATURES, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerEncoder(EMBEDDING_DIM, NUM_HEADS, FFN_DIM)\n",
    "    x = transformer_block(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(DROPOUT)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(DROPOUT)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = create_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining some callbacks for better control of model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "\n",
    "# Define TensorBoard callback\n",
    "log_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model_name = 'transformer_model.h5' \n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    model_name,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "learning_rate_decay = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    factor=0.2,\n",
    "    min_lr=0.00001,\n",
    "    verbose=1\n",
    ")  \n",
    "\n",
    "callbacks = [checkpoint, early_stopping, learning_rate_decay, tensorboard]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = transformer_model.fit(\n",
    "    train_data,\n",
    "    train_label,    \n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.evaluate(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_utils import plot_history_metrics, get_classification_report \n",
    "plot_history_metrics(history, ['loss', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_classification_report(transformer_model, test_data, test_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "The model isn't generalizing all to good, which can be seen by the increasing validation loss, which is an indicator for overfitting. To combat this we could lower the vocabulary size, decrease sequence length or even try pre-trained embeddings like GloVe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-trained word embeddings (GloVe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [GloVe](https://github.com/stanfordnlp/GloVe) pre-trained embeddings which were published by the Stanford University. The 100d Model is available on [Kaggle](https://www.kaggle.com/datasets/anindya2906/glove6b). The file `glove.6B.100d.txt` needs to be placed in the `embeddings` folder so that the path `embeddings/glove.6B.100d.txt` is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_utils import read_embeddings\n",
    "filepath = './embeddings/glove.6B.100d.txt'\n",
    "GLOVE_EMBEDDINGS = read_embeddings(filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the encoding of a random word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word = 'hello'\n",
    "test_vector = GLOVE_EMBEDDINGS[test_word]\n",
    "print(f'Vector for {test_word}:\\n\\n{test_vector}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All vectors have 100 dimensions to capture the semenatics of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Vector shape: {test_vector.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode our vocabulary with GloVe Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MATRIX = np.zeros((MAX_FEATURES, SEQ_LENGTH))\n",
    "\n",
    "for i, word in np.ndenumerate(vocabulary):\n",
    "    embedding_vector = GLOVE_EMBEDDINGS.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        EMBEDDING_MATRIX[i] = embedding_vector\n",
    "\n",
    "print(f'Embedding matrix shape: {EMBEDDING_MATRIX.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model with glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_transformer_model = create_model(EMBEDDING_MATRIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_history = transformer_model.fit(\n",
    "    train_data,\n",
    "    train_label,    \n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_transformer_model.evaluate(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history_metrics(history, ['loss', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
