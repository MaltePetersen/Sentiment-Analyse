{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analyse von diversen Reviewdaten durch die Nutzung von Wordembeddings und LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zielsetzung: \n",
    "In diesem Notebook untersuchen wir, ob moderne Methoden wie Word Embedding und LSTMs unsere Genauigkeit weiter verbessern können. Hierfür verwenden wir erneut die Kombination aus McDonald's und IMDB-Bewertungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 13:11:41.658206: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-09 13:11:41.684419: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-09 13:11:42.073542: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from utils.text_utils import clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/zsh: /home/x/miniconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /usr/bin/zsh)\n",
      "Requirement already satisfied: kaggle in /home/x/miniconda3/envs/tf/lib/python3.9/site-packages (1.5.13)\n",
      "Requirement already satisfied: six>=1.10 in /home/x/miniconda3/envs/tf/lib/python3.9/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi in /home/x/miniconda3/envs/tf/lib/python3.9/site-packages (from kaggle) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil in /home/x/miniconda3/envs/tf/lib/python3.9/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in /home/x/miniconda3/envs/tf/lib/python3.9/site-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /home/x/miniconda3/envs/tf/lib/python3.9/site-packages (from kaggle) (4.65.0)\n",
      "Requirement already satisfied: python-slugify in /home/x/miniconda3/envs/tf/lib/python3.9/site-packages (from kaggle) (8.0.1)\n",
      "Requirement already satisfied: urllib3 in /home/x/miniconda3/envs/tf/lib/python3.9/site-packages (from kaggle) (1.26.15)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/x/miniconda3/envs/tf/lib/python3.9/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/x/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->kaggle) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/x/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->kaggle) (3.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install kaggle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduzierte Ausführungszeit durch lokale Ausführung\n",
    "Da nicht jeder über eine leistungsstarke Grafikkarte mit GPU-Unterstützung verfügt, kann die Ausführung von LSTMs zeitaufwändig sein. Eine effektive Alternative zur lokalen Ausführung bietet Google Colab. Dieser Dienst ermöglicht die kostenlose Ausführung des Codes mit GPU-Unterstützung. Obwohl die Authentifizierung bei Diensten wie Kaggle etwas komplexer sein kann, haben wir in den Notebooks Tools integriert, die eine einfache Authentifizierung in Google Colab ermöglichen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentifizierung bei Kaggle\n",
    "Navigieren Sie zu https://www.kaggle.com. Gehen Sie dann zu Ihrem [Benutzerprofils](https://www.kaggle.com/me/account) und wählen Sie \"API-Token erstellen\" aus. Dadurch wird die Datei kaggle.json heruntergeladen, die Ihre API-Zugangsdaten enthalten.\n",
    "\n",
    "Führen Sie anschließend die nachstehende Zelle aus, um kaggle.json in Ihrer Colab-Laufzeit hochzuladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m files\n\u001b[1;32m      3\u001b[0m uploaded \u001b[39m=\u001b[39m files\u001b[39m.\u001b[39mupload()\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m fn \u001b[39min\u001b[39;00m uploaded\u001b[39m.\u001b[39mkeys():\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "    \n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "            name=fn, length=len(uploaded[fn])))\n",
    "\n",
    "# Then move kaggle.json into the folder where the API expects to find it.\n",
    "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Herunterladen der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Download McDonalds dataset\n",
    "if not os.path.exists('mcdonalds-store-reviews.zip'):\n",
    "    print(\"Downloading McDonalds dataset...\")\n",
    "    !kaggle datasets download -d nelgiriyewithana/mcdonalds-store-reviews\n",
    "if os.path.exists('mcdonalds-store-reviews.zip'):\n",
    "    print(\"Unzipping McDonalds dataset...\")\n",
    "    !unzip -n mcdonalds-store-reviews.zip\n",
    "\n",
    "# Download IMDB dataset\n",
    "if not os.path.exists('imdb-dataset-of-50k-movie-reviews.zip'):\n",
    "    print(\"Downloading IMDB dataset...\")\n",
    "    !kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "if os.path.exists('imdb-dataset-of-50k-movie-reviews.zip'):\n",
    "    print(\"Unzipping IMDB dataset...\")\n",
    "    !unzip -n imdb-dataset-of-50k-movie-reviews.zip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prozessierung der Daten\n",
    "Die Prozessierung wird nicht weiterbeschrieben, weil sie identisch zum letzten Notebook ist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mc = pd.read_csv('McDonald_s_Reviews.csv', encoding=\"latin-1\")\n",
    "df_imdb = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mc = df_mc[['review', 'rating']]\n",
    "df_imdb = df_imdb[['review', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why does it look like someone spit on my food?...</td>\n",
       "      <td>1 star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It'd McDonalds. It is what it is as far as the...</td>\n",
       "      <td>4 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Made a mobile order got to the speaker and che...</td>\n",
       "      <td>1 star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My mc. Crispy chicken sandwich was ï¿½ï¿½ï¿½ï¿...</td>\n",
       "      <td>5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I repeat my order 3 times in the drive thru, a...</td>\n",
       "      <td>1 star</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review   rating\n",
       "0  Why does it look like someone spit on my food?...   1 star\n",
       "1  It'd McDonalds. It is what it is as far as the...  4 stars\n",
       "2  Made a mobile order got to the speaker and che...   1 star\n",
       "3  My mc. Crispy chicken sandwich was ï¿½ï¿½ï¿½ï¿...  5 stars\n",
       "4  I repeat my order 3 times in the drive thru, a...   1 star"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mc['review'] = df_mc['review'].apply(clean_text)\n",
    "df_imdb['review'] = df_imdb['review'].apply(clean_text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>look like someone spit food normal transaction...</td>\n",
       "      <td>1 star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>itd mcdonalds far food atmosphere go staff mak...</td>\n",
       "      <td>4 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>made mobile order got speaker checked line mov...</td>\n",
       "      <td>1 star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mc crispy chicken sandwich customer service qu...</td>\n",
       "      <td>5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>repeat order times drive thru still manage mes...</td>\n",
       "      <td>1 star</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review   rating\n",
       "0  look like someone spit food normal transaction...   1 star\n",
       "1  itd mcdonalds far food atmosphere go staff mak...  4 stars\n",
       "2  made mobile order got speaker checked line mov...   1 star\n",
       "3  mc crispy chicken sandwich customer service qu...  5 stars\n",
       "4  repeat order times drive thru still manage mes...   1 star"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewers mentioned watching oz episode yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production br br filming tech...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically theres family little boy jake thinks...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one reviewers mentioned watching oz episode yo...  positive\n",
       "1  wonderful little production br br filming tech...  positive\n",
       "2  thought wonderful way spend time hot summer we...  positive\n",
       "3  basically theres family little boy jake thinks...  negative\n",
       "4  petter matteis love time money visually stunni...  positive"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mc = df_mc[df_mc['rating'] != '3 stars']\n",
    "data_mc = df_mc['review'].to_numpy()\n",
    "data_imdb = df_imdb['review'].to_numpy()\n",
    "rating_mapping_imdb = {\n",
    "    'positive': 1,\n",
    "    'negative': 0,\n",
    "}\n",
    "\n",
    "label_imdb = df_imdb['sentiment'].map(rating_mapping_imdb).to_numpy()\n",
    "rating_mapping_mc = {\n",
    "    '1 star': 0,\n",
    "    '2 stars': 0,\n",
    "    '4 stars': 1,\n",
    "    '5 stars': 1\n",
    "}\n",
    "\n",
    "label_mc = df_mc['rating'].map(rating_mapping_mc).to_numpy()\n",
    "data = np.append(data_imdb, data_mc)\n",
    "label = np.append(label_imdb,label_mc)\n",
    "\n",
    "train_data, test_data, train_label, test_label = train_test_split(data, label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generierung eines Sequence Models aus den Daten\n",
    "### 1. Text zu Intergers\n",
    "Um ein Sequenze Model zu erstellen werden die Input Samples als erstes in Sequenzen von Integer Indizes konvertiert, wobei ein Integer ein Wort repräsentiert.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization_sequence = TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    "    standardize=None,\n",
    ")\n",
    "text_vectorization_sequence.adapt(train_data)\n",
    "text_vectorization_sequence.adapt(test_data)\n",
    "\n",
    "int_train_ds = text_vectorization_sequence(train_data)\n",
    "\n",
    "int_test_ds = text_vectorization_sequence(test_data)\n",
    "\n",
    "vocabulary = np.array(text_vectorization_sequence.get_vocabulary())\n",
    "\n",
    "print(\"Vocabulary size: {}\".format(len(vocabulary)))\n",
    "print(\"Vocabulary content:\\n {}\".format(vocabulary[:20]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Konvertierung der Integer Sequenzen in Vektor Sequenzen\n",
    "### 2a. One Hot Encoding\n",
    "Ein erster Ansatz zur Konvertierung der Integersequenzen in Vektorsequenzen war die Verwendung des One-Hot-Encodings. Dabei würde jede Dimension ein Wort im Vokabular repräsentieren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "  keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",  # Metric to monitor\n",
    "        patience=3,  # Number of epochs with no improvement after which training will be stopped\n",
    "        restore_best_weights=True,  # Restore the weights of the best epoch\n",
    "    )\n",
    "]\n",
    "\n",
    "oh_history = model.fit(int_train_ds, train_label, epochs=10,validation_split=0.2,callbacks=callbacks)\n",
    "\n",
    "model.evaluate(int_test_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Model trainiert nur sehr langsam oder stürzt bei manchen PCs sogar ab, weil zu wenig RAM vorhanden ist. One-Hot Encoding ist klar die falsche Lösung um diese Daten in ein Vektorsequenzen zu transformieren."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Word-Embeddings\n",
    "Ein wesentliches Problem des One-Hot-Encodings war, dass die Repräsentation keine Zusammenhänge zwischen den Wörtern abbildete, da sie unabhängig voneinander waren. Jedoch sind Wörter nicht unabhängig voneinander. Um diese Abhängigkeiten darzustellen, können Word Embeddings verwendet werden. In diesen reflektieren die geometrischen Beziehungen zwischen zwei Vektoren auch ihre semantischen Beziehungen. \n",
    "\n",
    "Keras stellt uns bereits Embedding-Layer zur Verfügung, die wir nutzen können. Diese funktionieren so, dass ihre Gewichtungen (ein internes Dictionary von Token-Vektoren) anfangs zufällig sind. Während des Trainings werden diese Vektoren durch Backpropagation schrittweise angepasst. Die Struktur dieser Layer kann dann von anderen Layern genutzt werden, um die Leistung zu verbessern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    " loss=\"binary_crossentropy\",\n",
    " metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Was ist ein RNNs und LSTMs?\n",
    "RNNs (Recurrent Neural Networks) sind eine Art von neuronalen Netzwerken, die speziell für die Verarbeitung von sequenziellen Daten entwickelt wurden. Im Gegensatz zu herkömmlichen neuronalen Netzwerken können RNNs Informationen über vergangene Zustände behalten und sie in die Berechnung aktueller Schritte einbeziehen.\n",
    "\n",
    "Durch ihre Fähigkeit, Kontextinformationen über vergangene Eingaben zu erfassen, können RNNs komplexe Abhängigkeiten in sequenziellen Daten modellieren und lernen. Deshalb können sie auch auf NLP Probleme angewendet werden.\n",
    "\n",
    "Eine der großen Herausforderungen von RNNs ist das \"Vanishing Gradient Problem\". Es beschreibt das Verschwinden oder die Explosion des Gradienten während des Backpropagation-Algorithmus, der zum Trainieren des Netzwerks verwendet wird.\n",
    "\n",
    "Das Vanishing Gradient Problem tritt auf, wenn die Ableitungsfunktionen in den Schichten des Netzwerks Werte kleiner als 1 haben, insbesondere wenn sie nahe bei 0 liegen. In diesem Fall wird der Gradient in jeder Schicht während der Rückpropagierung mit jedem Schritt weiter multipliziert, wodurch er exponentiell kleiner wird. Als Ergebnis erhalten die Gewichte in den ersten Schichten des Netzwerks nur sehr kleine Aktualisierungen, was zu langsamerem oder gar keinem Lernen führen kann. Dies beeinträchtigt insbesondere RNNs, die auf lange Sequenzen angewendet werden.\n",
    "\n",
    "LSTMs reduzieren dieses Problem durch ihre Architektur. Im Vergleich zu herkömmlichen RNNs verwenden LSTMs einen zusätzlichen Zellzustand, der als \"Gedächtnis\" fungiert und Informationen über längere Sequenzen hinweg speichert. Der Zellzustand ermöglicht es den LSTMs, wichtige Informationen beizubehalten und irrelevante Informationen zu verwerfen, was dazu beiträgt, das Verschwinden des Gradienten zu verhindern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "  keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",  # Metric to monitor\n",
    "        patience=3,  # Number of epochs with no improvement after which training will be stopped\n",
    "        restore_best_weights=True,  # Restore the weights of the best epoch\n",
    "    )\n",
    "]\n",
    "model.fit(int_train_ds, train_label,validation_split=0.2\n",
    ", epochs=10,\n",
    " callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(int_test_ds, test_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warum sind die Ergbnisse signfikant schlechter als mit dem Bag of Words Model? \n",
    "Wir sind noch ein Stück von den Ergebnissen des simpleren Bigram-Modells entfernt. Ein Teil des Grundes dafür ist einfach, dass das Modell etwas weniger Daten betrachtet: Das Bigram-Modell verarbeitete vollständige Bewertungen, während unser Sequenzmodell Sequenzen nach 600 Wörtern abschneidet.\n",
    "Es gibt hierfür aber noch weitere Gründe. Diese werden im letzen Abschnitt des Notebooks erläutert."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ansehen der Word-Embedding Daten\n",
    "Um die entstandenen geometrischen Beziehungen zu betrachten, die die semantischen Beziehungen zwischen den Vektoren zeigen, kann das Tool \"TensorFlow Projector\" verwendet werden. Dazu müssen lediglich die Daten aus dem nächsten Code-Block über den folgenden Link hochgeladen werden: http://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = text_vectorization_sequence.get_vocabulary()\n",
    "\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nutzen von vordefinierten Embeddings\n",
    "Nun nutzen wir die [GloVe](https://github.com/stanfordnlp/GloVe) embeddings von der Stanford NLP Arbeitsgruppe.\n",
    "Es wird das 100d Model genutzt welches auf Kaggle ([hier](https://www.kaggle.com/datasets/anindya2906/glove6b)) heruntergeladen werden kann. Anschliessend muss dieses in den `embeddings` ordner gelegt werden, sodass `embeddings/glove.6B.100d.txt` zur Verfuegung steht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_utils import read_embeddings\n",
    "filepath = './embeddings/glove.6B.100d.txt'\n",
    "GLOVE_EMBEDDINGS = read_embeddings(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word = 'hello'\n",
    "test_vector = GLOVE_EMBEDDINGS[test_word]\n",
    "print(f\"Vektor des Wortes '{test_word}' sieht wie folgt aus:\\n\\n{test_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Vector shape: {test_vector.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "EMBEDDINGS_MATRIX = np.zeros((max_tokens, embedding_dim))\n",
    "\n",
    "for i, word in np.ndenumerate(vocabulary):\n",
    "    embedding_vector = GLOVE_EMBEDDINGS.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        EMBEDDINGS_MATRIX[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, embedding_dim, maxlen, embedding_matrix):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=False),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, dropout=0.5)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        verbose=1,\n",
    "        patience=5,\n",
    "        restore_best_weights=True)\n",
    "    \n",
    "    callbacks = [early_stopping]    \n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy']) \n",
    "\n",
    "    return model, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, callbacks = create_model(max_tokens, embedding_dim, max_length, EMBEDDINGS_MATRIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(int_train_ds, train_label, validation_split=0.2, epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(int_test_ds, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_utils import plot_history_metrics\n",
    "plot_history_metrics(history, ['loss', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_utils import get_classification_report\n",
    "get_classification_report(model,int_train_ds, train_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warum performen die Sequence Model Lösungen nicht unbedingt besser als die bag of Gram Lösungen?\n",
    "Obwohl die Bag-of-Words-Methode (BoW) nicht mehr die aktuellste Technologie im Natural Language Processing (NLP) ist, bietet sie für bestimmte Textklassifikationsaufgaben weiterhin die überlegene Lösung. \n",
    "\n",
    "Das Google Keras Team empfiehlt als Heuristik, das Verhältnis zwischen der Anzahl der Samples in den Trainingsdaten und der durchschnittlichen Anzahl von Wörtern pro Sample zu betrachten. Wenn dieses Verhältnis über 1500 liegt, ist ein Sequence Model die geeignetere Lösung, während bei einem Verhältnis unter 1500 ein Bigram die bessere Wahl ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = []\n",
    "for item in data: \n",
    "    words = item.split()\n",
    "    word_counts.append(len(words))\n",
    "\n",
    "word_counts =  np.array(word_counts)\n",
    "\n",
    "\n",
    "ratio = len(word_counts) / word_counts.mean()\n",
    "print(ratio)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei unseren Daten ist diese Ratio 505.64, deshalb können wir klar sagen, dass Bigrams der bessere Ansatz für die Sentiment Analyse von Reviews dieser Wortanzahl ist. \n",
    "\n",
    "Durch das Hinzufügen der IMDB-Trainingsdaten haben wir versucht, ein allgemeingültigeres Modell zu erstellen. Jedoch ist diese Form der Generalisierung mit einem Bigram-Ansatz nur bis zu einer bestimmten Menge an Trainingsdaten effektiv. Wenn wir unsere Trainingsdaten um weitere 500.000 Amazon Reviews erweitern würden und sich der Durchschnitt (Mean) dabei nicht ändern würde, wäre die Verwendung von Sequence Encodings die richtige Wahl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withAmazon = (len(word_counts) + 500000 ) /  word_counts.mean()\n",
    "print(withAmazon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
