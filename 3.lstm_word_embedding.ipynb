{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analyse von diversen Reviewdaten durch die Nutzung von Wordembeddings und LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zielsetzung: \n",
    "In diesem Notebook untersuchen wir, ob moderne Methoden wie Word Embedding und LSTMs unsere Genauigkeit weiter verbessern können. Hierfür verwenden wir erneut die Kombination aus McDonald's und IMDB-Bewertungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kaggle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduzierte Ausführungszeit durch lokale Ausführung\n",
    "Da nicht jeder über eine leistungsstarke Grafikkarte mit GPU-Unterstützung verfügt, kann die Ausführung von LSTMs zeitaufwändig sein. Eine effektive Alternative zur lokalen Ausführung bietet Google Colab. Dieser Dienst ermöglicht die kostenlose Ausführung des Codes mit GPU-Unterstützung. Obwohl die Authentifizierung bei Diensten wie Kaggle etwas komplexer sein kann, haben wir in den Notebooks Tools integriert, die eine einfache Authentifizierung in Google Colab ermöglichen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentifizierung bei Kaggle\n",
    "Navigieren Sie zu https://www.kaggle.com. Gehen Sie dann zu Ihrem [Benutzerprofils](https://www.kaggle.com/me/account) und wählen Sie \"API-Token erstellen\" aus. Dadurch wird die Datei kaggle.json heruntergeladen, die Ihre API-Zugangsdaten enthalten.\n",
    "\n",
    "Führen Sie anschließend die nachstehende Zelle aus, um kaggle.json in Ihrer Colab-Laufzeit hochzuladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))\n",
    "  \n",
    "# Then move kaggle.json into the folder where the API expects to find it.\n",
    "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Herunterladen der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Download McDonalds dataset\n",
    "if not os.path.exists('mcdonalds-store-reviews.zip'):\n",
    "    print(\"Downloading McDonalds dataset...\")\n",
    "    !kaggle datasets download -d nelgiriyewithana/mcdonalds-store-reviews\n",
    "if os.path.exists('mcdonalds-store-reviews.zip'):\n",
    "    print(\"Unzipping McDonalds dataset...\")\n",
    "    !unzip -n mcdonalds-store-reviews.zip\n",
    "\n",
    "# Download IMDB dataset\n",
    "if not os.path.exists('imdb-dataset-of-50k-movie-reviews.zip'):\n",
    "    print(\"Downloading IMDB dataset...\")\n",
    "    !kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "if os.path.exists('imdb-dataset-of-50k-movie-reviews.zip'):\n",
    "    print(\"Unzipping IMDB dataset...\")\n",
    "    !unzip -n imdb-dataset-of-50k-movie-reviews.zip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prozessierung der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mc = pd.read_csv('McDonald_s_Reviews.csv', encoding=\"latin-1\")\n",
    "df_imdb = pd.read_csv('IMDB Dataset.csv')\n",
    "df_mc = df_mc[df_mc['rating'] != '3 stars']\n",
    "data_mc = df_mc['review'].to_numpy()\n",
    "data_imdb = df_imdb['review'].to_numpy()\n",
    "rating_mapping_imdb = {\n",
    "    'positive': 1,\n",
    "    'negative': 0,\n",
    "}\n",
    "\n",
    "label_imdb = df_imdb['sentiment'].map(rating_mapping_imdb).to_numpy()\n",
    "rating_mapping_mc = {\n",
    "    '1 star': 0,\n",
    "    '2 stars': 0,\n",
    "    '4 stars': 1,\n",
    "    '5 stars': 1\n",
    "}\n",
    "\n",
    "label_mc = df_mc['rating'].map(rating_mapping_mc).to_numpy()\n",
    "data = np.append(data_imdb, data_mc)\n",
    "label = np.append(label_imdb,label_mc)\n",
    "\n",
    "train_data, test_data, train_label, test_label = train_test_split(data, label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generierung eines Sequence Models aus den Daten\n",
    "### 1. Text zu Intergers\n",
    "Um ein Sequenze Model zu erstellen werden die Input Samples als erstes in Sequenzen von Integer Indizes konvertiert, wobei ein Integer ein Wort repräsentiert.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization_sequence = TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "text_vectorization_sequence.adapt(train_data)\n",
    "text_vectorization_sequence.adapt(test_data)\n",
    "\n",
    "int_train_ds = text_vectorization_sequence(train_data)\n",
    "\n",
    "int_test_ds = text_vectorization_sequence(test_data)\n",
    "\n",
    "vocabulary = np.array(text_vectorization_sequence.get_vocabulary())\n",
    "\n",
    "print(\"Vocabulary size: {}\".format(len(vocabulary)))\n",
    "print(\"Vocabulary content:\\n {}\".format(vocabulary[:20]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Konvertierung der Integer Sequenzen in Vektor Sequenzen\n",
    "### 2a. One Hot Encoding\n",
    "Ein erster Ansatz zur Konvertierung der Integersequenzen in Vektorsequenzen war die Verwendung des One-Hot-Encodings. Dabei würde jede Dimension ein Wort im Vokabular repräsentieren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "  keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",  # Metric to monitor\n",
    "        patience=3,  # Number of epochs with no improvement after which training will be stopped\n",
    "        restore_best_weights=True,  # Restore the weights of the best epoch\n",
    "    )\n",
    "]\n",
    "\n",
    "oh_history = model.fit(int_train_ds, train_label, epochs=10,validation_split=0.2,callbacks=callbacks)\n",
    "\n",
    "model.evaluate(int_test_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Model trainiert nur sehr langsam oder stürzt bei manchen PCs sogar ab, weil zu wenig RAM vorhanden ist. One-Hot Encoding ist klar die falsche Lösung um diese Daten in ein Vektorsequenzen zu transformieren."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Word-Embeddings\n",
    "Ein wesentliches Problem des One-Hot-Encodings war, dass die Repräsentation keine Zusammenhänge zwischen den Wörtern abbildete, da sie unabhängig voneinander waren. Jedoch sind Wörter nicht unabhängig voneinander. Um diese Abhängigkeiten darzustellen, können Word Embeddings verwendet werden. In diesen reflektieren die geometrischen Beziehungen zwischen zwei Vektoren auch ihre semantischen Beziehungen. \n",
    "\n",
    "Keras stellt uns bereits Embedding-Layer zur Verfügung, die wir nutzen können. Diese funktionieren so, dass ihre Gewichtungen (ein internes Dictionary von Token-Vektoren) anfangs zufällig sind. Während des Trainings werden diese Vektoren durch Backpropagation schrittweise angepasst. Die Struktur dieser Layer kann dann von anderen Layern genutzt werden, um die Leistung zu verbessern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    " loss=\"binary_crossentropy\",\n",
    " metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "  keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",  # Metric to monitor\n",
    "        patience=3,  # Number of epochs with no improvement after which training will be stopped\n",
    "        restore_best_weights=True,  # Restore the weights of the best epoch\n",
    "    )\n",
    "]\n",
    "model.fit(int_train_ds, train_label,validation_split=0.4\n",
    ", epochs=10,\n",
    " callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(int_test_ds, test_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warum sind die Ergbnisse signfikant schlechter als mit dem Bag of Words Model? \n",
    "Wir sind noch ein Stück von den Ergebnissen des simpleren Bigram-Modells entfernt. Ein Teil des Grundes dafür ist einfach, dass das Modell etwas weniger Daten betrachtet: Das Bigram-Modell verarbeitete vollständige Bewertungen, während unser Sequenzmodell Sequenzen nach 600 Wörtern abschneidet.\n",
    "Es gibt hierfür aber noch weitere Gründe. Diese werden im letzen Abschnitt des Notebooks erläutert."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ansehen der Word-Embedding Daten\n",
    "Um die entstandenen geometrischen Beziehungen zu betrachten, die die semantischen Beziehungen zwischen den Vektoren zeigen, kann das Tool \"TensorFlow Projector\" verwendet werden. Dazu müssen lediglich die Daten aus dem nächsten Code-Block über den folgenden Link hochgeladen werden: http://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = text_vectorization_sequence.get_vocabulary()\n",
    "\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nutzen von vordefinierten Embeddings\n",
    "Nun nutzen wir die [GloVe](https://github.com/stanfordnlp/GloVe) embeddings von der Stanford NLP Arbeitsgruppe.\n",
    "Es wird das 100d Model genutzt welches auf Kaggle ([hier](https://www.kaggle.com/datasets/anindya2906/glove6b)) heruntergeladen werden kann. Anschliessend muss dieses in den `embeddings` ordner gelegt werden, sodass `embeddings/glove.6B.100d.txt` zur Verfuegung steht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_utils import read_embeddings\n",
    "filepath = './embeddings/glove.6B.100d.txt'\n",
    "GLOVE_EMBEDDINGS = read_embeddings(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word = 'hello'\n",
    "test_vector = GLOVE_EMBEDDINGS[test_word]\n",
    "print(f\"Vektor des Wortes '{test_word}' sieht wie folgt aus:\\n\\n{test_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Vector shape: {test_vector.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "EMBEDDINGS_MATRIX = np.zeros((max_tokens, embedding_dim))\n",
    "\n",
    "for i, word in np.ndenumerate(vocabulary):\n",
    "    embedding_vector = GLOVE_EMBEDDINGS.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        EMBEDDINGS_MATRIX[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, embedding_dim, maxlen, embedding_matrix):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=False),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, dropout=0.5)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        verbose=1,\n",
    "        patience=5,\n",
    "        restore_best_weights=True)\n",
    "    \n",
    "    callbacks = [early_stopping]    \n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy']) \n",
    "\n",
    "    return model, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, callbacks = create_model(max_tokens, embedding_dim, max_length, EMBEDDINGS_MATRIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(int_train_ds, train_label, validation_split=0.2, epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(int_test_ds, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_utils import plot_history_metrics\n",
    "plot_history_metrics(history, ['loss', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_utils import get_classification_report\n",
    "get_classification_report(model,int_train_ds, train_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warum performen die Sequence Model Lösungen nicht unbedingt besser als die bag of Gram Lösungen?\n",
    "Obwohl die Bag-of-Words-Methode (BoW) nicht mehr die aktuellste Technologie im Natural Language Processing (NLP) ist, bietet sie für bestimmte Textklassifikationsaufgaben weiterhin die überlegene Lösung. \n",
    "\n",
    "Das Google Keras Team empfiehlt als Heuristik, das Verhältnis zwischen der Anzahl der Samples in den Trainingsdaten und der durchschnittlichen Anzahl von Wörtern pro Sample zu betrachten. Wenn dieses Verhältnis über 1500 liegt, ist ein Sequence Model die geeignetere Lösung, während bei einem Verhältnis unter 1500 ein Bigram die bessere Wahl ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = []\n",
    "for item in data: \n",
    "    words = item.split()\n",
    "    word_counts.append(len(words))\n",
    "\n",
    "word_counts =  np.array(word_counts)\n",
    "\n",
    "\n",
    "ratio = len(word_counts) / word_counts.mean()\n",
    "print(ratio)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei unseren Daten ist diese Ratio 505.64, deshalb können wir klar sagen, dass Bigrams der bessere Ansatz für die Sentiment Analyse von Reviews dieser Wortanzahl ist. \n",
    "\n",
    "Durch das Hinzufügen der IMDB-Trainingsdaten haben wir versucht, ein allgemeingültigeres Modell zu erstellen. Jedoch ist diese Form der Generalisierung mit einem Bigram-Ansatz nur bis zu einer bestimmten Menge an Trainingsdaten effektiv. Wenn wir unsere Trainingsdaten um weitere 500.000 Amazon Reviews erweitern würden und sich der Durchschnitt (Mean) dabei nicht ändern würde, wäre die Verwendung von Sequence Encodings die richtige Wahl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withAmazon = (len(word_counts) + 500000 ) /  word_counts.mean()\n",
    "print(withAmazon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
